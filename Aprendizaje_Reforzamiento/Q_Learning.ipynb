{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7eb345-df55-4e74-8511-fbd4d3d0c0f0",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "El Q-learning es un algoritmo de aprendizaje por refuerzo basado en la idea de aprendizaje mediante ensayo y error. Su principal objetivo es descubrir la estrategia óptima que guía las acciones del agente para maximizar el valor esperado de las recompensas futuras. El agente aprende a estimar el valor de cada acción posible en un estado específico.  \n",
    "\n",
    "Estos valores se almacenan en una tabla conocida como Q-table, un mapa que asocia cada estado con todas las acciones posibles y sus respectivos valores de utilidad, es decir, la ganancia esperada por el agente cuando realiza una determinada acción en un estado específico. Luego, el agente utiliza estos valores de utilidad para seleccionar acciones óptimas en función de cada situación. \n",
    "\n",
    "El Q-learning sigue un proceso de aprendizaje iterativo y se basa en la ecuación de Bellman, que expresa el valor óptimo de una política como la suma de las recompensas inmediatas y el valor esperado de las recompensas futuras. Esta ecuación es fundamental para calcular los valores de utilidad en la Q-table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc27de-b773-4654-9538-a4664fc988d5",
   "metadata": {},
   "source": [
    "## Juego de Laberinto\n",
    "Haremos que nuestro robot se mueva en un laberinto simple para llegar a un objetivo de la mejor forma posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6a3a23-a442-48a0-832c-f4979905e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46b6828-afdb-4d6b-a1c8-eafbd79b4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_dimensions = (5,5)\n",
    "started_point = (0,0)\n",
    "target_point = (4,4)\n",
    "\n",
    "# Declaramos coordenadas donde colocaremos obstaculos dentro del tablero\n",
    "obstacles = [(1,1), (1,3), (2,3), (3,0)]\n",
    "\n",
    "# Declaramos los movimientos validos de nuestro \"agente\"\n",
    "actions = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "\n",
    "# Calculamos la catidad de casillas dentro del tablero\n",
    "num_of_points = board_dimensions[0] * board_dimensions[1]\n",
    "\n",
    "# Cantidad de acciones que puede llevar a cabo nuestro agente\n",
    "num_actions = len(actions)\n",
    "\n",
    "Q = np.zeros((num_of_points, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77c449e-d6a9-455c-bdf5-7a6df6294fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos funcion que nos permita obtener el indice de cada casilla para nuestra comprension\n",
    "def position2index(position):\n",
    "    return position[0] * board_dimensions[1] + position[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97db0b4-b4b4-497e-a432-7ff1d90b2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los parametros de aprendizaje\n",
    "alpha = 0.5    # Factor de tasa de aprendizaje\n",
    "gamma = 0.99   # Factor de descuento (importancia de las recompensas a futuro)\n",
    "epsilon = 0.2  # Probabilidad de tomar una accion aleatoria\n",
    "epocas = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac618a9a-fcd4-4f81-a864-f96d3054299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(position):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        return np.argmax(Q[casilla_a_indice(casilla)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "554ffda9-76ff-4749-9c6f-d8168532fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice = random.choice(actions)\n",
    "choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
